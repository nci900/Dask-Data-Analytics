{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95db8e3e",
   "metadata": {},
   "source": [
    "Scale Scikit-Learn for Small Data Problems\n",
    "==========================================\n",
    "\n",
    "Dask can be used to scale scikit-learn to a cluster of machines for a CPU-bound problem.\n",
    "We will be using a local cluster with 4 workers, each with 1 thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2470f61-0dc2-4159-8f19-8a6198d5c92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b62f497-dfbf-436f-8780-444883ae6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df9dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4, threads_per_worker=1, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fd0618",
   "metadata": {},
   "source": [
    "## Distributed Training\n",
    "\n",
    "\n",
    "Scikit-learn uses [joblib](http://joblib.readthedocs.io/) for single-machine parallelism. This lets you train most estimators (anything that accepts an `n_jobs` parameter) using all the cores of your laptop or workstation.\n",
    "\n",
    "Alternatively, Scikit-Learn can use Dask for parallelism.  This lets you train those estimators using all the cores of your *cluster* without significantly changing your code.  This is most useful for training large models on medium-sized datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0a6574",
   "metadata": {},
   "source": [
    "### Create Scikit-Learn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657644f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ffe486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Up: set categories=None to use all the categories\n",
    "categories = [\n",
    "    'alt.atheism',\n",
    "    'talk.religion.misc',\n",
    "]\n",
    "\n",
    "print(\"Loading 20 newsgroups dataset for categories:\")\n",
    "print(categories)\n",
    "\n",
    "data = fetch_20newsgroups(subset='train', categories=categories)\n",
    "print(\"%d documents\" % len(data.filenames))\n",
    "print(\"%d categories\" % len(data.target_names))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aa31d3",
   "metadata": {},
   "source": [
    "We'll define a small pipeline that combines text feature extraction with a simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e79a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', HashingVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter=1000)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f51b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5359ff0d",
   "metadata": {},
   "source": [
    "### Define Grid for Parameter Search\n",
    "Grid search over some parameters.\n",
    "GridSearchCV is a technique for finding the optimal parameter values from a given set of parameters in a grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d091bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    # 'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__n_iter': (10, 50, 80),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69b6553",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=3, refit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d3bdd",
   "metadata": {},
   "source": [
    "To fit this normally, we would write\n",
    "\n",
    "\n",
    "```python\n",
    "grid_search.fit(data.data, data.target)\n",
    "```\n",
    "\n",
    "That would use the default joblib backend (multiple processes) for parallelism.\n",
    "To use the Dask distributed backend, which will use a cluster of machines to train the model, perform the fit in a `parallel_backend` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9566fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "with joblib.parallel_backend('dask'):\n",
    "    grid_search.fit(data.data, data.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defca903",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05faa8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63570ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "cv_results = pd.DataFrame(grid_search.cv_results_)\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c218eb",
   "metadata": {},
   "source": [
    "Score and Predict Large Datasets\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa047d62",
   "metadata": {},
   "source": [
    "Sometimes you'll train on a smaller dataset that fits in memory, but need to predict or score for a much larger (possibly larger than memory) dataset.\n",
    "Perhaps your [learning curve](http://scikit-learn.org/stable/modules/learning_curve.html) has leveled off, or you only have labels for a subset of the data.\n",
    "\n",
    "In this situation, you can use [ParallelPostFit](http://ml.dask.org/modules/generated/dask_ml.wrappers.ParallelPostFit.html) to parallelize and distribute the scoring or prediction steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3781f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client, progress\n",
    "\n",
    "# Scale up: connect to your own cluster with more resources\n",
    "# see http://dask.pydata.org/en/latest/setup.html\n",
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcc6872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import dask.array as da\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813b6333",
   "metadata": {},
   "source": [
    "We'll generate a small random dataset with scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29362fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2,\n",
    "    random_state=1, n_clusters_per_class=1, n_samples=1000)\n",
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d2291a",
   "metadata": {},
   "source": [
    "And we'll clone that dataset many times with `dask.array`. `X_large` and `y_large` represent our larger than memory dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef76566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up: increase N, the number of times we replicate the data.\n",
    "N = 100\n",
    "X_large = da.concatenate([da.from_array(X_train, chunks=X_train.shape)\n",
    "                          for _ in range(N)])\n",
    "y_large = da.concatenate([da.from_array(y_train, chunks=y_train.shape)\n",
    "                          for _ in range(N)])\n",
    "X_large"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb06d9d5",
   "metadata": {},
   "source": [
    "Since our training dataset fits in memory, we can use a scikit-learn estimator as the actual estimator fit during traning.\n",
    "But we know that we'll want to predict for a large dataset, so we'll wrap the scikit-learn estimator with `ParallelPostFit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffec09e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from dask_ml.wrappers import ParallelPostFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcada8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = ParallelPostFit(LogisticRegressionCV(cv=3), scoring=\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1fb715",
   "metadata": {},
   "source": [
    "See the note in the `dask-ml`'s documentation about when and why a `scoring` parameter is needed: https://ml.dask.org/modules/generated/dask_ml.wrappers.ParallelPostFit.html#dask_ml.wrappers.ParallelPostFit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b72d807",
   "metadata": {},
   "source": [
    "Now we'll call `clf.fit`. Dask-ML does nothing here, so this step can only use datasets that fit in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581fb268",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c256b791",
   "metadata": {},
   "source": [
    "Now that training is done, we'll turn to predicting for the full (larger than memory) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36854bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_large)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d6190",
   "metadata": {},
   "source": [
    "`y_pred` is a Dask array.\n",
    "Workers can write the predicted values to a shared file system, without ever having to collect the data on a single machine.\n",
    "\n",
    "Or we can check the models score on the entire large dataset.\n",
    "The computation will be done in parallel, and no single machine will have to hold all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5e1047",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_large, y_large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e41c51",
   "metadata": {},
   "source": [
    "Incrementally Train Large Datasets\n",
    "=================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a1571c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=4, threads_per_worker=1)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86580125",
   "metadata": {},
   "source": [
    "## Create Data\n",
    "\n",
    "We create a synthetic dataset that is large enough to be interesting, but small enough to run quickly.  \n",
    "\n",
    "Our dataset has 1,000,000 examples and 100 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3e80f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "from dask_ml.datasets import make_classification\n",
    "\n",
    "\n",
    "n, d = 100000, 100\n",
    "\n",
    "X, y = make_classification(n_samples=n, n_features=d,\n",
    "                           chunks=n // 10, flip_y=0.2)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04869e2",
   "metadata": {},
   "source": [
    "## Split data for training and testing\n",
    "\n",
    "We split our dataset into training and testing data to aid evaluation by making sure we have a fair test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32914ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad94723",
   "metadata": {},
   "source": [
    "## Persist data in memory\n",
    "\n",
    "This dataset is small enough to fit in distributed memory, so we call `dask.persist` to ask Dask to execute the computations above and keep the results in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66010c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = dask.persist(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068f98b1",
   "metadata": {},
   "source": [
    "If you are working in a situation where your dataset does not fit in memory then you should skip this step.  Everything will still work, but will be slower and use less memory.\n",
    "\n",
    "Calling `dask.persist` will preserve our data in memory, so no computation will be needed as we pass over our data many times.  For example if our data came from CSV files and was not persisted, then the CSV files would have to be re-read on each pass.  This is desirable if the data does not fit in RAM, but not slows down our computation otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c10ecf",
   "metadata": {},
   "source": [
    "## Precompute classes\n",
    "\n",
    "We pre-compute the classes from our training data, which is required for this classification example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cc09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = da.unique(y_train).compute()\n",
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741d6d9a",
   "metadata": {},
   "source": [
    "## Create Scikit-Learn model\n",
    "\n",
    "We make the underlying Scikit-Learn estimator, an `SGDClassifier`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4def5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "est = SGDClassifier(loss='squared_error', penalty='l2', tol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72455d6",
   "metadata": {},
   "source": [
    "Here we use `SGDClassifier`, but any estimator that implements the `partial_fit` method will work.  A list of Scikit-Learn models that implement this API is available [here](https://scikit-learn.org/stable/computing/scaling_strategies.html#incremental-learning).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8397d38",
   "metadata": {},
   "source": [
    "## Wrap with Dask-ML's Incremental meta-estimator\n",
    "\n",
    "We now wrap our `SGDClassifer` with the [`dask_ml.wrappers.Incremental`](http://ml.dask.org/modules/generated/dask_ml.wrappers.Incremental.html#dask_ml.wrappers.Incremental) meta-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7111dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.wrappers import Incremental\n",
    "\n",
    "inc = Incremental(est, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f28ebd",
   "metadata": {},
   "source": [
    "`Incremental` only does data management while leaving the actual algorithm to the underlying Scikit-Learn estimator.\n",
    "\n",
    "Note: We set the scoring parameter above in the Dask estimator to tell it to handle scoring.  This works better when using Dask arrays for test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe1cdfd",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "`Incremental` implements a `fit` method, which will perform one loop over the dataset, calling `partial_fit` over each chunk in the Dask array.\n",
    "\n",
    "You may want to watch the dashboard during this fit process to see the sequential fitting of many batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6cf50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.fit(X_train, y_train, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf00a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4c5d16",
   "metadata": {},
   "source": [
    "Train Models on Large Datasets\n",
    "==============================\n",
    "\n",
    "Most estimators in scikit-learn are designed to work with NumPy arrays or scipy sparse matricies.\n",
    "These data structures must fit in the RAM on a single machine.\n",
    "\n",
    "Estimators implemented in Dask-ML work well with Dask Arrays and DataFrames. This can be much larger than a single machine's RAM. They can be distributed in memory on a cluster of machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d124a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "# Scale up: connect to your own cluster with more resources\n",
    "# see http://dask.pydata.org/en/latest/setup.html\n",
    "client = Client(processes=False, threads_per_worker=4,\n",
    "                n_workers=1, memory_limit='2GB')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask_ml.datasets\n",
    "import dask_ml.cluster\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca89243",
   "metadata": {},
   "source": [
    "In this example, we'll use `dask_ml.datasets.make_blobs` to generate some random *dask* arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25820e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale up: increase n_samples or n_features\n",
    "X, y = dask_ml.datasets.make_blobs(n_samples=1000000,\n",
    "                                   chunks=100000,\n",
    "                                   random_state=0,\n",
    "                                   centers=3)\n",
    "X = X.persist()\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f917974c",
   "metadata": {},
   "source": [
    "We'll use the k-means implemented in Dask-ML to cluster the points. It uses the `k-means||` (read: \"k-means parallel\") initialization algorithm, which scales better than `k-means++`. All of the computation, both during and after initialization, can be done in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b9f9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "km = dask_ml.cluster.KMeans(n_clusters=3, init_max_iter=2, oversampling_factor=10)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0844d5",
   "metadata": {},
   "source": [
    "We'll plot a sample of points, colored by the cluster each falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e0f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(X[::1000, 0], X[::1000, 1], marker='.', c=km.labels_[::1000],\n",
    "           cmap='viridis', alpha=0.25);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
