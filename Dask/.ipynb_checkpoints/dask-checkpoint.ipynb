{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "244dd0ca",
   "metadata": {},
   "source": [
    "![dask.png](dask.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4275f2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import LocalCluster, Client\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.array as da\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "427862e0-8bf0-4e53-bf42-e75e45dfd95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the workshop directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f9df83",
   "metadata": {},
   "source": [
    "### Dask Collections\n",
    "\n",
    "* **High-level collections**: Mimic NumPy, lists, and pandas but can operate in parallel on datasets that don’t fit into memory \n",
    "    * Array\n",
    "    * DataFrame\n",
    "    * Bag\n",
    "    \n",
    "* **Low-level collections**: Give finer control to build custom parallel and distributed computations\n",
    "    * Delayed\n",
    "    * Futures\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f229f3",
   "metadata": {},
   "source": [
    "# Dask Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba32bea",
   "metadata": {},
   "source": [
    "![dataframe.png](dataframe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817a4c6e",
   "metadata": {},
   "source": [
    "* One Dask DataFrame is comprised of many in-memory pandas DataFrames separated along the index. \n",
    "* One operation on a Dask DataFrame triggers many pandas operations on the constituent pandas DataFrames \n",
    "* These operations are mindful of potential parallelism and memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c1b5259-0320-4a90-9395-cd42892f0335",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db7fbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'data/nycflights/*.csv': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls data/nycflights/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99d43b5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: /scratch/vp91/jj8451Dask-Data-Analytics/data/nycflights/*.csv resolved to no files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/dataframe/io/csv.py:761\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    749\u001b[0m     urlpath,\n\u001b[1;32m    750\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    760\u001b[0m ):\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/dataframe/io/csv.py:533\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murlpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m resolved to no files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Infer compression from first path\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: /scratch/vp91/jj8451Dask-Data-Analytics/data/nycflights/*.csv resolved to no files",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read all the csv file into a single Dask dataframe\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDask-Data-Analytics/data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnycflights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: /scratch/vp91/jj8451Dask-Data-Analytics/data/nycflights/*.csv resolved to no files"
     ]
    }
   ],
   "source": [
    "# Read all the csv file into a single Dask dataframe\n",
    "ddf = dd.read_csv(\n",
    "    os.path.join(path+\"/Dask-Data-Analytics/data\", \"nycflights\", \"*.csv\"), parse_dates={\"Date\": [0, 1, 2]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265cc6cc",
   "metadata": {},
   "source": [
    "* dask.dataframe.read_csv only reads in a sample from the beginning of the file\n",
    "* These inferred datatypes are then enforced when reading all partitions\n",
    "* Sometimes, datatypes inferred in the sample can be incorrect. \n",
    "    * The first n rows have no value for CRSElapsedTime (which pandas infers as a float), and later on turn out to be strings (object dtype). \n",
    "\n",
    "* Good practice - specify dtypes directly using the dtype keyword. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a594d497",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: data/nycflights/*.csv resolved to no files",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/backends.py:136\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/dataframe/io/csv.py:761\u001b[0m, in \u001b[0;36mmake_reader.<locals>.read\u001b[0;34m(urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\n\u001b[1;32m    749\u001b[0m     urlpath,\n\u001b[1;32m    750\u001b[0m     blocksize\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    760\u001b[0m ):\n\u001b[0;32m--> 761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mread_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    762\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    763\u001b[0m \u001b[43m        \u001b[49m\u001b[43murlpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    764\u001b[0m \u001b[43m        \u001b[49m\u001b[43mblocksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblocksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    765\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    766\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    767\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    768\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_rows\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43massume_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43massume_missing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m        \u001b[49m\u001b[43minclude_path_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_path_column\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    774\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/dataframe/io/csv.py:533\u001b[0m, in \u001b[0;36mread_pandas\u001b[0;34m(reader, urlpath, blocksize, lineterminator, compression, sample, sample_rows, enforce, assume_missing, storage_options, include_path_column, **kwargs)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(paths) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00murlpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m resolved to no files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Infer compression from first path\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: data/nycflights/*.csv resolved to no files",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ddf \u001b[38;5;241m=\u001b[39m \u001b[43mdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnycflights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTailNum\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCRSElapsedTime\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCancelled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/vp91/AAPP2023/dask-python3.9-venv/lib/python3.9/site-packages/dask/backends.py:138\u001b[0m, in \u001b[0;36mCreationDispatch.register_inplace.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname(func)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod registered to the \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m backend.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Message: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: An error occurred while calling the read_csv method registered to the pandas backend.\nOriginal Message: data/nycflights/*.csv resolved to no files"
     ]
    }
   ],
   "source": [
    "ddf = dd.read_csv(\n",
    "    os.path.join(\"data\", \"nycflights\", \"*.csv\"),\n",
    "    parse_dates={\"Date\": [0, 1, 2]},\n",
    "    dtype={\"TailNum\": str, \"CRSElapsedTime\": float, \"Cancelled\": bool},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dff91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755783fc",
   "metadata": {},
   "source": [
    "### Lazy evaluation\n",
    "* Representation of the DataFrame object contains no data \n",
    "* Dask has just done enough to read the start of the first file, and infer the column names and dtypes\n",
    "\n",
    "* Dask **constructs** the logic (called task graph) of your computation immediately\n",
    "* **Evaluates** them only when necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87245763",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cf78a9",
   "metadata": {},
   "source": [
    "* Functions like len, head, tail also trigger an evaluation.\n",
    "    * load actual data, (that is, load each file into a pandas DataFrame)\n",
    "    * apply the corresponding functions to each pandas DataFrame (also known as a partition)\n",
    "    * combine the subtotals to give you the final grand total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a98b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadb7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb19168",
   "metadata": {},
   "source": [
    "### Operation on multiple files in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ab5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# find the max value of the DepDelay coulmn in all the 10 dataframes\n",
    "files = os.listdir(os.path.join('data', 'nycflights'))\n",
    "maxes = []\n",
    "\n",
    "for file in files:\n",
    "    df = pd.read_csv(os.path.join('data', 'nycflights', file))\n",
    "    maxes.append(df.DepDelay.max())\n",
    "\n",
    "final_max = max(maxes)\n",
    "print(final_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f3c18e",
   "metadata": {},
   "source": [
    "### Operation on multiple files in Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b28325b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the max value of the DepDelay coulmn in all the 10 dataframes\n",
    "# This only creates the task graph, it does not execute the operation\n",
    "result = ddf.DepDelay.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8658b303",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1dd17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9affd4",
   "metadata": {},
   "source": [
    "### Excercise: Find the number of flight from each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc668f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46c3b859",
   "metadata": {},
   "source": [
    "* We can also combine multiple compute steps into a single instruction\n",
    "* This is usualy more efficient\n",
    "    * Task graphs for both results are merged when calling dask.compute\n",
    "    * shared operations to only be done once instead of twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb276f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_canceled = ddf[~ddf.Cancelled]\n",
    "mean_delay = non_canceled.DepDelay.mean()\n",
    "std_delay = non_canceled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf48cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf26d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f4dab9",
   "metadata": {},
   "source": [
    "# Dask  Arrays - parallelized numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed090fb",
   "metadata": {},
   "source": [
    "![arrays.png](arrays.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcec49c",
   "metadata": {},
   "source": [
    "* Dask Array implements a subset of the NumPy ndarray interface using **blocked** algorithms\n",
    "* Large array is cut into many small arrays\n",
    "* Large computations are performed by combining many smaller computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091c95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NumPy array\n",
    "a_np = np.ones(10)\n",
    "a_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0631cd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how a blocked operation is done in numpy. We divide the whole ndarray\n",
    "# of size 10 int slices of 2, each of size 5\n",
    "\n",
    "a_np_sum = a_np[:5].sum() + a_np[5:].sum()\n",
    "a_np_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b125d053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask array\n",
    "\n",
    "# In task ndarray we specify the slices usinh the keyword chunk. \n",
    "# chunk defines the numer of elements in each slice\n",
    "\n",
    "a_da = da.ones(10, chunks=5)\n",
    "a_da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ca50c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum = a_da.sum()\n",
    "a_da_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b866c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e919e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_da_sum.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e65fc0b",
   "metadata": {},
   "source": [
    "* Dask can also find an optimal chunk by itself\n",
    "* If your chunks are too small\n",
    "    * the amount of actual work done by every task is very tiny\n",
    "    * the overhead of coordinating all these tasks results in a very inefficient process\n",
    "* If your chunks are too big\n",
    "    * you will likely run out of memory\n",
    "    * data will have to be moved to the disk \n",
    "    * this will lead to performance decrements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4985f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000), chunks=(3000, 3000)) # We specify the chunk\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d124a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "xd = da.random.normal(10, 0.1, size=(30_000, 30_000)) # Dask finds the chunk\n",
    "yd = xd.mean(axis=0)\n",
    "yd.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986eb1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xd.chunksize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ac0a72",
   "metadata": {},
   "source": [
    "# Delayed decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d04a",
   "metadata": {},
   "source": [
    "* A Block of code can have operations that can happen in parallel\n",
    "* Normally in python these operation will happen sequentially\n",
    "    * Or the user will identify the parallel section and write parallel codes\n",
    "* The Dask **delayed** function decorates your functions so that they operate lazily \n",
    "* Dask will defer execution of the function, placing the function and its arguments into a task graph\n",
    "* Dask will then identify oppurtunities for parallelism in the task graph\n",
    "* The Dask schedulers will exploit this parallelism, generally improving performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fca403",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def inc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78533de",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def add(x, y):\n",
    "    time.sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb953074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the two increments are independent of each other, we can run them in parallel\n",
    "\n",
    "x = inc(1)\n",
    "y = inc(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e28b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here Z is a delayed object\n",
    "\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b43f964",
   "metadata": {},
   "outputs": [],
   "source": [
    "z.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9acd56",
   "metadata": {},
   "source": [
    "# Dask future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8137e5c4",
   "metadata": {},
   "source": [
    "* we can submit individual functions for evaluation\n",
    "* The call returns immediately, giving one or more future\n",
    "    * whose status begins as “pending”\n",
    "    * later becomes “finished”\n",
    "* There is no **blocking** of the local Python session.\n",
    "\n",
    "* Difference between futures and delayed\n",
    "    * delayed is lazy (it just constructs a graph) \n",
    "    * futures are eager. \n",
    "    * With futures, as soon as the inputs are available and there is compute available, the computation starts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61751b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client(n_workers=4)\n",
    "\n",
    "def inc(x):\n",
    "    time.sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "\n",
    "def double(x):\n",
    "    sleep(2)\n",
    "    return 2 * x\n",
    "\n",
    "\n",
    "def add(x, y):\n",
    "    time.sleep(1)\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = client.submit(inc, 1)  # returns immediately with pending future\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98103a85",
   "metadata": {},
   "source": [
    "If we check the future after a few seconds we can see that it is complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07115235",
   "metadata": {},
   "outputs": [],
   "source": [
    "future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183e5903",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2e497a-367b-4202-a943-435ff21670ee",
   "metadata": {},
   "source": [
    "## Compute Vs Persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2d291e-946f-4614-8f86-1d81c4287969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dask.datasets.timeseries()\n",
    "df.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbea6e-cc5b-4151-9fff-8fb24dceb1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c780d77a-af72-4680-8c4c-b7c65a9c0a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "computed_df = df.compute()\n",
    "type(computed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c7cf48-b48f-4b4c-9cf2-0c9703edc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persist = df.persist()\n",
    "type(df_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d55f0-e9c1-4448-bce3-945c31b1a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persist.npartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafe47e1-5ae6-4297-97d3-f247d34d2e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
